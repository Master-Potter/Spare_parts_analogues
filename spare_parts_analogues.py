# -*- coding: utf-8 -*-
"""Spare parts analogues

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14pCSIsnL0_2_Um4TqRA2zAXyRJnCTbbD

**Определение аналогов и дубликатов на складе с помощью нейросети**
"""

# Подгружаем гугл-диск
from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from tensorflow import keras
from tensorflow.keras.optimizers import Adam # 
from tensorflow.keras.preprocessing.text import Tokenizer
import tensorflow as tf
from tensorflow.keras.utils import plot_model
from tensorflow.keras.layers import LSTM, GRU, SimpleRNN, Bidirectional, Reshape, Concatenate

from tensorflow.keras.optimizers import Adam # 
from tensorflow.keras import utils # Для работы с категориальными данными
from tensorflow.keras.models import Sequential, Model # Полносвязная модель
from tensorflow.keras.layers import Dense, Dropout, SpatialDropout1D, BatchNormalization, Embedding, Flatten, Activation, Input, concatenate # Слои для сети
from tensorflow.keras.preprocessing.text import Tokenizer # Методы для работы с текстами и преобразования их в последовательности
from tensorflow.keras.preprocessing.sequence import pad_sequences # Метод для работы с последовательностями
 
from sklearn.preprocessing import LabelEncoder # Метод кодирования тестовых лейблов
from sklearn.model_selection import train_test_split # Для разделения выборки на тестовую и обучающую
from google.colab import drive # Для работы с Google Drive
import time # Импортируем библиотеку time
from scipy.stats import mode
import time

# Статический вывод графики (графики отображаются в той ячейке, в которой используется plt.show())
# %matplotlib inline

"""**ПОДГОТОВКА БАЗЫ**"""

warehouse = pd.read_csv ("/content/drive/MyDrive/Analog&duplicat/SpareParts2.csv", sep = '\t')#Загружаем csv-файл с данными

warehouse.head(10)#Проверяем загрузку по 10 первым строкам

print(warehouse.shape)#Печатаем струкуру массива (количество строк: 663, столбцов: 15)

warehouse.fillna('$', inplace=True) #Переименовываем NaN в "$"

warehouse = warehouse.astype(str).apply(lambda x: x.str.lower())#Преобразовываем наш DataFrame к нижнему регистру

warehouse.drop(['Unnamed: 13','Unnamed: 14'], axis =1, inplace=True) # Убираем 13, 14-е столбцы. Они нам не нужны

warehouse.head(10)

#ФОРМИРУЕМ ОСИ
x = warehouse['Material'] #Первый столбец - системный номер запасной части в складском учёте предприятия..str.lower()
x1 = warehouse['Short text'] # Второй столбец - краткое наименование запасной части
y = ['SKF', 'FAG', 'SNR', 'KOYO', 'EZO', 'NTN', 'NSK', 'RHP', 'ROLLWAY', 'TIMKEN', 'INA']#Столбцы с аналогами-заменителями по брендам.

df_n = warehouse[y].to_numpy() #Преобразуем наш dataframe в массив NumPy
len_df = [[len(df_n[i,j]) for i in range(663)] for j in range(11)]
plt.hist(len_df)

max_len_id = np.max(len_df)#Получаем максимальное количество символов в ячейке
max_len_id

warehouse = warehouse.astype("U21")#Данные массива содержат как целые числа, так и строки, поэтому приравниваем данные к одному astype ("U21")

warehouse_ = warehouse.to_numpy()

[' '.join(x) for x in warehouse_]#Сцепляет строковые представления объектов из массива, перед преобразованием в индексы.

# Производим токенизацию текста, т.е. преобразуем буквы и цифры в индексы (в токены)
tokenizer = Tokenizer(filters = '', char_level=True) #, oov_token='<unk>' #Токенизация производится на уровне символов: char_level=True.
tokenizer.fit_on_texts([' '.join(x) for x in warehouse_]) # "Скармливаем" наши тексты, т. е. даём в обработку методу, который соберет словарь частотности
tokenizer.word_index #Вывод словаря частотности

len(tokenizer.word_index)

y_train = [] #Создаём список тренировочной выборки  

for i in y:
    temp = tokenizer.texts_to_sequences(warehouse[i])
    pad_temp = pad_sequences(temp, maxlen=max_len_id)
    y_train.append(pad_temp)
y_train = np.array(y_train)

#Формируем размеры y-train-массива
y_train.shape

y_train

#Меняем оси местами, т.е. делаем сперва строки, затем колонки и количество символов в ячейках
y_train_ = np.transpose(y_train, (1,0,2)) 
y_train_.shape

#Преобрабатываем данные для модели, 11*33 (количество y-столбуов на количество символов в ячейке):
yTrainScaled = y_train_.reshape((y_train_.shape[0],len(y)*max_len_id))#11*33 
yTrainScaled.shape

#Получаем то есть закодированные слова в числа
yTrainScaled

#Количество символов в ячейках по осям
xTrainScaled = tokenizer.texts_to_sequences(x.astype('U21'))
max_len_x = np.max([len(i) for i in xTrainScaled])
xTrainScaled = pad_sequences(xTrainScaled, maxlen=max_len_x)
xTrainC01 = tokenizer.texts_to_sequences(x1)
max_len_x1 = np.max([len(i) for i in xTrainC01])
xTrainC01 = pad_sequences(xTrainC01, maxlen=max_len_x1)
print(xTrainScaled.shape)
print(xTrainC01.shape)
print(yTrainScaled.shape)

splitVal = 0.2 #Процент, который выделяем в проверочную выборку
valMask = np.random.sample(xTrainScaled.shape[0]) < splitVal #Создаём маску True-False для создания проверочной выборки

yTrainScaled = utils.to_categorical(yTrainScaled, len(tokenizer.word_index)) #Преобразует вектор класса (целые числа) в двоичную матрицу класса.
yTrainScaled.shape



#Формируем модель, Dense сеть:
input1 = Input((xTrainScaled.shape[1],))
input2 = Input((xTrainC01.shape[1],))

x1 = Dense(15, activation="relu")(input1)
x2 = Dense(15, activation="relu")(input2)

x_c = Concatenate(axis=-1)([x1,x2])

x = Dense(15, activation='relu')(x_c)
x = Dense(yTrainScaled.shape[1]*len(tokenizer.word_index), activation='relu')(x)
x_r = Reshape((yTrainScaled.shape[1], len(tokenizer.word_index)))(x)

x = Dense(len(tokenizer.word_index), activation='softmax')(x_r)

model = Model((input1, input2), x)


model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])
history = model.fit([xTrainScaled[~valMask], xTrainC01[~valMask]], 
                    yTrainScaled[~valMask], 
                    epochs=150, 
                    validation_data=([xTrainScaled[valMask], xTrainC01[valMask]], 
                    yTrainScaled[valMask]), 
                    verbose=1)
# строим график для отображения динамики обучения и точности предсказания сети
plt.plot(history.history['accuracy'], 
         label='Доля дубликатов и аналогов на обучающем наборе')
plt.plot(history.history['val_accuracy'], 
         label='Доля дубликатов и аналогов на проверочном наборе')
plt.xlabel('Эпоха обучения')
plt.ylabel('Доля дубликатов и аналогов')
plt.legend()
plt.show()

"""**Визуализация модели. На вход подаём массив с символами X1, X2 и Y для предсказания.**"""

model.summary()
plot_model
plot_model(model, to_file='model.png')

"""**Для проверки предсказания используем функцию:**"""

#Преобразовываем индексы в текст по xTrain: 
pred_ = model.predict([xTrainScaled[valMask], xTrainC01[valMask]])
pred = np.argmax(pred_, axis=-1)
pred = pred.reshape(pred.shape[0], 11, 33) #Проводим reshape перед токенизацией
str_ = np.array([tokenizer.sequences_to_texts(i) for i in pred])#Преобразуем индексы в текст
rez = pd.DataFrame(str_, columns=y)
rez

#Преобразовываем индексы в текст по yTrain: 
pred_1 = np.argmax(yTrainScaled[valMask], axis=-1)
str_1 = pred_1.reshape(pred_1.shape[0],11,33)#Проводим reshape перед токенизацией
str_1 = np.array([tokenizer.sequences_to_texts(i) for i in str_1])#Преобразуем индексы в текст по каждой колонке
y_true = pd.DataFrame(str_1, columns=y)
y_true

#Сконтенерируем все строчки в одну
' '.join(str_[0])

#Сконтенерируем все строчки в одну по yTrain
' '.join(str_1[0])

!pip -q install jiwer #репозиторий для автоматической оценки сходства между двумя строками

from jiwer import wer # модуль метрики качества в распознавании речи

#Подсчёт средней (mean) и медианы(median) по потерям(loss) 
loss = []
for i in range(len(str_1)):
    loss.append(wer(' '.join(str_[i]).lower(), ' '.join(str_1[i]).lower()))#Подсчёт результата с помощью Wer, между Str_1 фактом (True) и предсказанием Str_

loss = np.array(loss)
print('Loss mean:', loss.mean())
print('Loss median:', np.median(loss))

# было mean 2.1079671785554135
# было median 1.6363636363636365

#Выведем сравнительные строки между y-train True и predict
k = 0
v = 0
for i, s_ in enumerate(str_1):
        for j, s in enumerate(s_):
            if s != "$":
                print('y_train:     ', s)
                v +=1
                if str_[i][j] != '$':
                    k+-1
                print('pred:        ', str_[i][j])
                print()